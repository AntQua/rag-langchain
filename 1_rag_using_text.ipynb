{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./text_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Chunking: Structuring Data for LLMs\n",
    "Breaking large text blocks into smaller, meaningful segments.\n",
    "\n",
    "The chunking logic includes a custom separator (\"\\n\") to ensure chunks stay within the defined size while maintaining semantic coherence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of text files in the directory\n",
    "files = os.listdir(data_dir)\n",
    "# Initialize a list to store chunked texts\n",
    "file_texts = []\n",
    "\n",
    "# Loop through all files and process them\n",
    "for file in files:\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "\n",
    "    # Open the file and read its content\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        file_text = f.read()\n",
    "\n",
    "    # Create a CharacterTextSplitter object with specified chunk size, overlap, and separator \n",
    "    text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=128, chunk_overlap=32, separator=\"\\n\" \n",
    "    )\n",
    "\n",
    "    # Split the text into chunks\n",
    "    texts = text_splitter.split_text(file_text)\n",
    "\n",
    "    # Convert each chunk into a Document object with metadata\n",
    "    for i, chunked_text in enumerate(texts):\n",
    "        file_texts.append(Document(page_content=chunked_text,metadata={ \n",
    "                    \"doc_title\": file.split(\".\")[0],\n",
    "                    \"chunk_num\": i})) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the first chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Are you interested in joining our team at Big Star Collectibles? Take a look '\n",
      " 'at our job listings below and apply to what suits you best.\\n'\n",
      " 'If we don’t currently have a job available that you’re hoping to fill, you '\n",
      " 'can still apply. Just choose “Keep me in mind” as the job you’re applying '\n",
      " 'for.  \\n'\n",
      " 'We can’t wait to hear from you!\\n'\n",
      " 'Associate E-commerce Web Developer')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(file_texts[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Metadata: Enhancing RAG Applications\n",
    "Additional data stored alongside embeddings to enhance search and retrieval.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the first chunk and its metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chunk_num': 0, 'doc_title': 'Careers'}\n"
     ]
    }
   ],
   "source": [
    "pprint(file_texts[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Embeddings: Representing Data in Vector Form\n",
    "Representing data in vector form to capture its semantic meaning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include embedding and vector storage using FAISS and HuggingFaceEmbeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\") # embed your dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the embedded data into a vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_documents(\n",
    "    file_texts,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying\n",
    "Add the querying functionality using the as_retriever method and included an example query.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the vector store as a retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'doc_title': 'Our Story', 'chunk_num': 0}, page_content='Our story began at the International Arts Conference in 2013. Our founder, Saura Chen, a trained photographer, captured a series of candid images and portraits of the keynote speaker and presenters at the event, and provided print copies of the photographs to attendees at the end of the day. When she overheard a group of attendees attempting to secure autographs from the presenters and negotiating photo trades, the seeds for Big Star Collectibles were planted.'),\n",
       " Document(metadata={'doc_title': 'Our Story', 'chunk_num': 1}, page_content='Launched officially in 2014, Saura was determined to create high-quality trading cards that were desirable and valuable for the collecting community. Besides monthly releases for the casual collector, Big Star Collectibles also releases limited editions and one-of-a-kind items. \\nBig Star Collectibles has grown over the years to include memorabilia, contests, events, appraisals, and consultation services.'),\n",
       " Document(metadata={'doc_title': 'What We Do', 'chunk_num': 0}, page_content='We go to the far reaches of the galaxy to bring top quality, authentic, and rare collectibles right to your door. \\nDesign and Sell\\nThe most apparent of our activities is designing and selling collectibles that reflect what our customers enjoy and want. Our team of product designers analyze and speculate new collectibles based on customer and market feedback. And we love surprising you.\\nSearch and Broker\\nFor a fee, our experts can assist you in finding a particular Big Star Collectibles item that you have been looking for. Big Star Collectibles can also broker sales and trades among our customers.\\nEducate'),\n",
       " Document(metadata={'doc_title': 'Careers', 'chunk_num': 0}, page_content='Are you interested in joining our team at Big Star Collectibles? Take a look at our job listings below and apply to what suits you best.\\nIf we don’t currently have a job available that you’re hoping to fill, you can still apply. Just choose “Keep me in mind” as the job you’re applying for.  \\nWe can’t wait to hear from you!\\nAssociate E-commerce Web Developer')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"What year was Big Star Collectibles Started?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding the LLM \n",
    "Include the LLM integration with OpenAI, a prompt template, and a query chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate OpenAI LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "llm = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create prompt template.\n",
    "\n",
    "Include source citation in the prompt template. The LLM will explicitly cite the sources from the vector store metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "template=\"\"\"You are a helpful assistant. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Cite your sources.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the query chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke(\"When did Big Star Collectibles Launch? Cite where you found this information.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Big Star Collectibles launched officially in 2014. This information can be found in the context under the document \"Our Story\" and the chunk number 1. '"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
